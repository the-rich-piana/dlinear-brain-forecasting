Args in experiment:
Namespace(is_training=1, model_id='Informer_336_96', model='Informer', data='Activity', root_path='./dataset/', data_path='session_0.h5', features='M', target='OT', freq='m', checkpoints='./checkpoints/', seq_len=336, label_len=48, pred_len=96, individual=False, embed_type=0, enc_in=5000, dec_in=5000, c_out=5000, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=10, batch_size=64, patience=3, learning_rate=0.0005, des='Exp', loss='mse', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)
Use GPU: cuda:0
>>>>>>>start training : Informer_336_96_Informer_Activity_ftM_sl336_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Loaded preprocessed data: (19081, 6903)
Original neurons: 7673, Processed neurons: 6903
Data range: 0.000 to 1.000
Selected top 5000 neurons out of 6903 preprocessed neurons
Final dataset shape: (13356, 5000)
Final data statistics - Mean: 0.347, Std: 0.149
train 12925
Loaded preprocessed data: (19081, 6903)
Original neurons: 7673, Processed neurons: 6903
Data range: 0.000 to 1.000
Selected top 5000 neurons out of 6903 preprocessed neurons
Final dataset shape: (2244, 5000)
Final data statistics - Mean: 0.380, Std: 0.158
val 1813
Loaded preprocessed data: (19081, 6903)
Original neurons: 7673, Processed neurons: 6903
Data range: 0.000 to 1.000
Selected top 5000 neurons out of 6903 preprocessed neurons
Final dataset shape: (4153, 5000)
Final data statistics - Mean: 0.326, Std: 0.144
test 3722
	iters: 100, epoch: 1 | loss: 0.0221791
	speed: 0.3535s/iter; left time: 675.5541s
	iters: 200, epoch: 1 | loss: 0.0156840
	speed: 0.1958s/iter; left time: 354.5739s
Epoch: 1 cost time: 49.66014051437378
Epoch: 1, Steps: 201 | Train Loss: 0.0310367 Vali Loss: 0.0151819 Test Loss: 0.0125816
Validation loss decreased (inf --> 0.015182).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.0142780
	speed: 0.4525s/iter; left time: 773.8178s
	iters: 200, epoch: 2 | loss: 0.0137032
	speed: 0.1995s/iter; left time: 321.2525s
Epoch: 2 cost time: 41.25280833244324
Epoch: 2, Steps: 201 | Train Loss: 0.0144851 Vali Loss: 0.0150333 Test Loss: 0.0121375
Validation loss decreased (0.015182 --> 0.015033).  Saving model ...
Updating learning rate to 0.00025
	iters: 100, epoch: 3 | loss: 0.0137816
	speed: 0.4402s/iter; left time: 664.2009s
	iters: 200, epoch: 3 | loss: 0.0139939
	speed: 0.2005s/iter; left time: 282.5319s
Epoch: 3 cost time: 41.444305658340454
Epoch: 3, Steps: 201 | Train Loss: 0.0136298 Vali Loss: 0.0148255 Test Loss: 0.0119998
Validation loss decreased (0.015033 --> 0.014825).  Saving model ...
Updating learning rate to 0.000125
	iters: 100, epoch: 4 | loss: 0.0133663
	speed: 0.4378s/iter; left time: 572.6654s
	iters: 200, epoch: 4 | loss: 0.0130975
	speed: 0.1999s/iter; left time: 241.4490s
Epoch: 4 cost time: 41.309683322906494
Epoch: 4, Steps: 201 | Train Loss: 0.0133499 Vali Loss: 0.0147598 Test Loss: 0.0118942
Validation loss decreased (0.014825 --> 0.014760).  Saving model ...
Updating learning rate to 6.25e-05
	iters: 100, epoch: 5 | loss: 0.0137627
	speed: 0.4446s/iter; left time: 492.1578s
	iters: 200, epoch: 5 | loss: 0.0134556
	speed: 0.1992s/iter; left time: 200.5935s
Epoch: 5 cost time: 41.60489630699158
Epoch: 5, Steps: 201 | Train Loss: 0.0132338 Vali Loss: 0.0148321 Test Loss: 0.0119323
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.125e-05
	iters: 100, epoch: 6 | loss: 0.0130451
	speed: 0.4278s/iter; left time: 387.6265s
	iters: 200, epoch: 6 | loss: 0.0131221
	speed: 0.2001s/iter; left time: 161.2449s
Epoch: 6 cost time: 41.87823176383972
Epoch: 6, Steps: 201 | Train Loss: 0.0131827 Vali Loss: 0.0148070 Test Loss: 0.0120410
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.5625e-05
	iters: 100, epoch: 7 | loss: 0.0130100
	speed: 0.4226s/iter; left time: 297.9415s
	iters: 200, epoch: 7 | loss: 0.0130396
	speed: 0.2035s/iter; left time: 123.1168s
Epoch: 7 cost time: 41.757731914520264
Epoch: 7, Steps: 201 | Train Loss: 0.0131526 Vali Loss: 0.0147285 Test Loss: 0.0119351
Validation loss decreased (0.014760 --> 0.014729).  Saving model ...
Updating learning rate to 7.8125e-06
	iters: 100, epoch: 8 | loss: 0.0131013
	speed: 0.4450s/iter; left time: 224.2610s
	iters: 200, epoch: 8 | loss: 0.0133270
	speed: 0.1995s/iter; left time: 80.5835s
Epoch: 8 cost time: 41.31658458709717
Epoch: 8, Steps: 201 | Train Loss: 0.0131371 Vali Loss: 0.0147357 Test Loss: 0.0120009
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-06
	iters: 100, epoch: 9 | loss: 0.0131391
	speed: 0.4214s/iter; left time: 127.6983s
	iters: 200, epoch: 9 | loss: 0.0131343
	speed: 0.1995s/iter; left time: 40.4962s
Epoch: 9 cost time: 41.28430795669556
Epoch: 9, Steps: 201 | Train Loss: 0.0131277 Vali Loss: 0.0147437 Test Loss: 0.0119269
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.953125e-06
	iters: 100, epoch: 10 | loss: 0.0132859
	speed: 0.4199s/iter; left time: 42.8340s
	iters: 200, epoch: 10 | loss: 0.0129958
	speed: 0.1990s/iter; left time: 0.3981s
Epoch: 10 cost time: 41.18502187728882
Epoch: 10, Steps: 201 | Train Loss: 0.0131210 Vali Loss: 0.0147308 Test Loss: 0.0119341
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Informer_336_96_Informer_Activity_ftM_sl336_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Loaded preprocessed data: (19081, 6903)
Original neurons: 7673, Processed neurons: 6903
Data range: 0.000 to 1.000
Selected top 5000 neurons out of 6903 preprocessed neurons
Final dataset shape: (4153, 5000)
Final data statistics - Mean: 0.326, Std: 0.144
test 3722
mse:0.01193480845540762, mae:0.08026952296495438, rse:0.7636667490005493, corr:[0.11518813 0.11523271 0.11529133 0.11536133 0.11543594 0.11551262
 0.11350019 0.11357404 0.11340173 0.112746   0.11294934 0.11306679
 0.11591125 0.1159412  0.11597102 0.1159948  0.11601365 0.11603052
 0.11604685 0.11605608 0.11606672 0.11607744 0.11608779 0.11609406
 0.11425181 0.11413582 0.1138252  0.11331147 0.11326764 0.11329868
 0.11610315 0.11610448 0.11611982 0.11613983 0.11615651 0.11617988
 0.11619551 0.11622647 0.11623999 0.11626231 0.11627421 0.11630034
 0.11419144 0.1139224  0.11380152 0.1134873  0.11359803 0.11400353
 0.1164489  0.11644654 0.11644015 0.11643298 0.11643135 0.11642684
 0.11642584 0.11642516 0.11642642 0.11642464 0.11641594 0.11640406
 0.11435899 0.11417925 0.11396255 0.11335459 0.1136836  0.11358875
 0.11629586 0.11629865 0.1163022  0.11631062 0.11632873 0.11635026
 0.11636604 0.11637926 0.11638924 0.1164005  0.11640823 0.11641303
 0.11453443 0.11413252 0.11388725 0.11341839 0.11381552 0.11396291
 0.11662114 0.11667751 0.11673542 0.11679392 0.11683427 0.1168748
 0.11691853 0.11695427 0.11701247 0.11708648 0.11715526 0.11723525]
