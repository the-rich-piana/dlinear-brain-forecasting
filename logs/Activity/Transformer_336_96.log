Args in experiment:
Namespace(is_training=1, model_id='Transformer_336_96', model='Transformer', data='Activity', root_path='./dataset/', data_path='session_0.h5', features='M', target='OT', freq='m', checkpoints='./checkpoints/', seq_len=336, label_len=48, pred_len=96, individual=False, embed_type=0, enc_in=5000, dec_in=5000, c_out=5000, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0005, des='Exp', loss='mse', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)
Use GPU: cuda:0
>>>>>>>start training : Transformer_336_96_Transformer_Activity_ftM_sl336_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Loaded preprocessed data: (19081, 6903)
Original neurons: 7673, Processed neurons: 6903
Data range: 0.000 to 1.000
Selected top 5000 neurons out of 6903 preprocessed neurons
Final dataset shape: (13356, 5000)
Final data statistics - Mean: 0.347, Std: 0.149
train 12925
Loaded preprocessed data: (19081, 6903)
Original neurons: 7673, Processed neurons: 6903
Data range: 0.000 to 1.000
Selected top 5000 neurons out of 6903 preprocessed neurons
Final dataset shape: (2244, 5000)
Final data statistics - Mean: 0.380, Std: 0.158
val 1813
Loaded preprocessed data: (19081, 6903)
Original neurons: 7673, Processed neurons: 6903
Data range: 0.000 to 1.000
Selected top 5000 neurons out of 6903 preprocessed neurons
Final dataset shape: (4153, 5000)
Final data statistics - Mean: 0.326, Std: 0.144
test 3722
	iters: 100, epoch: 1 | loss: 0.0191965
	speed: 0.1220s/iter; left time: 479.4319s
	iters: 200, epoch: 1 | loss: 0.0178608
	speed: 0.1025s/iter; left time: 392.6961s
	iters: 300, epoch: 1 | loss: 0.0162059
	speed: 0.1027s/iter; left time: 383.2464s
	iters: 400, epoch: 1 | loss: 0.0150223
	speed: 0.1019s/iter; left time: 369.9034s
Epoch: 1 cost time: 41.96451234817505
Epoch: 1, Steps: 403 | Train Loss: 0.0217394 Vali Loss: 0.0171366 Test Loss: 0.0131556
Validation loss decreased (inf --> 0.017137).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.0139524
	speed: 0.3401s/iter; left time: 1199.7299s
	iters: 200, epoch: 2 | loss: 0.0134217
	speed: 0.1025s/iter; left time: 351.3759s
	iters: 300, epoch: 2 | loss: 0.0133457
	speed: 0.1027s/iter; left time: 341.8235s
	iters: 400, epoch: 2 | loss: 0.0131271
	speed: 0.1019s/iter; left time: 329.0264s
Epoch: 2 cost time: 41.830957651138306
Epoch: 2, Steps: 403 | Train Loss: 0.0135996 Vali Loss: 0.0152488 Test Loss: 0.0122295
Validation loss decreased (0.017137 --> 0.015249).  Saving model ...
Updating learning rate to 0.00025
	iters: 100, epoch: 3 | loss: 0.0131669
	speed: 0.3423s/iter; left time: 1069.7143s
	iters: 200, epoch: 3 | loss: 0.0136071
	speed: 0.1025s/iter; left time: 310.1929s
	iters: 300, epoch: 3 | loss: 0.0137343
	speed: 0.1026s/iter; left time: 300.1954s
	iters: 400, epoch: 3 | loss: 0.0127872
	speed: 0.1018s/iter; left time: 287.5927s
Epoch: 3 cost time: 41.84488582611084
Epoch: 3, Steps: 403 | Train Loss: 0.0131254 Vali Loss: 0.0148615 Test Loss: 0.0120363
Validation loss decreased (0.015249 --> 0.014862).  Saving model ...
Updating learning rate to 0.000125
	iters: 100, epoch: 4 | loss: 0.0123100
	speed: 0.4502s/iter; left time: 1225.4322s
	iters: 200, epoch: 4 | loss: 0.0123838
	speed: 0.1027s/iter; left time: 269.2159s
	iters: 300, epoch: 4 | loss: 0.0126558
	speed: 0.1033s/iter; left time: 260.5288s
	iters: 400, epoch: 4 | loss: 0.0123846
	speed: 0.1032s/iter; left time: 249.9997s
Epoch: 4 cost time: 42.16181492805481
Epoch: 4, Steps: 403 | Train Loss: 0.0128680 Vali Loss: 0.0147175 Test Loss: 0.0120337
Validation loss decreased (0.014862 --> 0.014718).  Saving model ...
Updating learning rate to 6.25e-05
	iters: 100, epoch: 5 | loss: 0.0128520
	speed: 0.3467s/iter; left time: 803.9058s
	iters: 200, epoch: 5 | loss: 0.0124453
	speed: 0.1057s/iter; left time: 234.4506s
	iters: 300, epoch: 5 | loss: 0.0124333
	speed: 0.1055s/iter; left time: 223.5971s
	iters: 400, epoch: 5 | loss: 0.0131810
	speed: 0.1029s/iter; left time: 207.6949s
Epoch: 5 cost time: 42.61028528213501
Epoch: 5, Steps: 403 | Train Loss: 0.0127642 Vali Loss: 0.0147654 Test Loss: 0.0120474
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.125e-05
	iters: 100, epoch: 6 | loss: 0.0129907
	speed: 0.3325s/iter; left time: 637.0733s
	iters: 200, epoch: 6 | loss: 0.0127691
	speed: 0.1039s/iter; left time: 188.7475s
	iters: 300, epoch: 6 | loss: 0.0128324
	speed: 0.1029s/iter; left time: 176.6090s
	iters: 400, epoch: 6 | loss: 0.0123070
	speed: 0.1027s/iter; left time: 166.0043s
Epoch: 6 cost time: 42.23010468482971
Epoch: 6, Steps: 403 | Train Loss: 0.0127397 Vali Loss: 0.0147005 Test Loss: 0.0120197
Validation loss decreased (0.014718 --> 0.014701).  Saving model ...
Updating learning rate to 1.5625e-05
	iters: 100, epoch: 7 | loss: 0.0127253
	speed: 0.3457s/iter; left time: 523.0292s
	iters: 200, epoch: 7 | loss: 0.0123109
	speed: 0.1023s/iter; left time: 144.4915s
	iters: 300, epoch: 7 | loss: 0.0124357
	speed: 0.1026s/iter; left time: 134.7148s
	iters: 400, epoch: 7 | loss: 0.0124775
	speed: 0.1018s/iter; left time: 123.5305s
Epoch: 7 cost time: 42.039522886276245
Epoch: 7, Steps: 403 | Train Loss: 0.0127293 Vali Loss: 0.0147017 Test Loss: 0.0120015
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.8125e-06
	iters: 100, epoch: 8 | loss: 0.0125513
	speed: 1.1654s/iter; left time: 1293.5744s
	iters: 200, epoch: 8 | loss: 0.0128396
	speed: 0.1023s/iter; left time: 103.2756s
	iters: 300, epoch: 8 | loss: 0.0132142
	speed: 0.1028s/iter; left time: 93.5344s
	iters: 400, epoch: 8 | loss: 0.0123506
	speed: 0.1017s/iter; left time: 82.3933s
Epoch: 8 cost time: 41.79583477973938
Epoch: 8, Steps: 403 | Train Loss: 0.0127215 Vali Loss: 0.0146924 Test Loss: 0.0119864
Validation loss decreased (0.014701 --> 0.014692).  Saving model ...
Updating learning rate to 3.90625e-06
	iters: 100, epoch: 9 | loss: 0.0125832
	speed: 0.3418s/iter; left time: 241.6391s
	iters: 200, epoch: 9 | loss: 0.0123149
	speed: 0.1031s/iter; left time: 62.5639s
	iters: 300, epoch: 9 | loss: 0.0128890
	speed: 0.1031s/iter; left time: 52.2643s
	iters: 400, epoch: 9 | loss: 0.0131944
	speed: 0.1022s/iter; left time: 41.5964s
Epoch: 9 cost time: 42.01959991455078
Epoch: 9, Steps: 403 | Train Loss: 0.0127185 Vali Loss: 0.0147047 Test Loss: 0.0120067
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-06
	iters: 100, epoch: 10 | loss: 0.0123748
	speed: 0.3247s/iter; left time: 98.7092s
	iters: 200, epoch: 10 | loss: 0.0133862
	speed: 0.1026s/iter; left time: 20.9365s
	iters: 300, epoch: 10 | loss: 0.0128927
	speed: 0.1027s/iter; left time: 10.6788s
	iters: 400, epoch: 10 | loss: 0.0124982
	speed: 0.1020s/iter; left time: 0.4080s
Epoch: 10 cost time: 41.845006704330444
Epoch: 10, Steps: 403 | Train Loss: 0.0127168 Vali Loss: 0.0146927 Test Loss: 0.0119856
EarlyStopping counter: 2 out of 3
Updating learning rate to 9.765625e-07
>>>>>>>testing : Transformer_336_96_Transformer_Activity_ftM_sl336_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Loaded preprocessed data: (19081, 6903)
Original neurons: 7673, Processed neurons: 6903
Data range: 0.000 to 1.000
Selected top 5000 neurons out of 6903 preprocessed neurons
Final dataset shape: (4153, 5000)
Final data statistics - Mean: 0.326, Std: 0.144
test 3722
mse:0.011986546218395233, mae:0.08084319531917572, rse:0.7653202414512634, corr:[0.11730535 0.11597419 0.11586282 0.11581555 0.11579052 0.11578611
 0.11578967 0.11578935 0.11577764 0.1157756  0.11577492 0.11577283
 0.11576954 0.1157616  0.11575409 0.1157382  0.11573187 0.11573238
 0.11573404 0.11573102 0.11574015 0.11573841 0.11573742 0.11574962
 0.11575176 0.11574592 0.11573462 0.11574432 0.11576339 0.11577056
 0.11577518 0.11577647 0.11576973 0.11576807 0.11576947 0.11578856
 0.11580677 0.11583049 0.11587033 0.11591294 0.11597874 0.1160473
 0.11612226 0.11618786 0.1162432  0.11628649 0.1163184  0.11636402
 0.11639156 0.1164067  0.11640458 0.11640985 0.11640437 0.11640786
 0.11641853 0.11643403 0.11644923 0.11646092 0.11645346 0.11645236
 0.11645359 0.11646228 0.11644854 0.11642952 0.11639296 0.11634658
 0.11632647 0.11630099 0.11627258 0.11624467 0.11624525 0.11624209
 0.11623524 0.11622375 0.11621889 0.11619228 0.11617038 0.11615309
 0.11614861 0.11616122 0.11618251 0.11620185 0.11623167 0.116251
 0.11626099 0.11629105 0.11631937 0.11634495 0.11634567 0.1163224
 0.116304   0.11629593 0.11631428 0.11633982 0.11637367 0.11621783]
