{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc098948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaccd22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Short\n",
    "resultsShort70 = pd.read_csv(\"ActivityShort70N.csv\")\n",
    "resultsShort700 = pd.read_csv(\"ActivityShort700N.csv\")\n",
    "resultsShort7000 = pd.read_csv(\"ActivityShort7000N.csv\")\n",
    "\n",
    "\n",
    "#Long\n",
    "resultsLong70 = pd.read_csv(\"ActivityLong70N.csv\")\n",
    "resultsLong700 = pd.read_csv(\"ActivityLong700N.csv\")\n",
    "resultsLong7000 = pd.read_csv(\"ActivityLong7000N.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2qprppwscvv",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating performance metrics for all datasets...\n",
      "==================================================\n",
      "Metrics calculated successfully!\n",
      "\n",
      "Formulas used:\n",
      "1. MAE Improvement %: ((Naive_MAE - Model_MAE) / Naive_MAE) × 100\n",
      "2. MSE Improvement %: ((Naive_MSE - Model_MSE) / Naive_MSE) × 100\n",
      "3. Prediction Score: 1 − L(model)/L(naive) where L = MSE loss\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate performance metrics relative to Naive baseline\n",
    "def calculate_performance_metrics(results_df, dataset_name):\n",
    "    \"\"\"\n",
    "    Calculate three performance metrics relative to Naive baseline:\n",
    "    1. MAE improvement %: ((Naive_MAE - Model_MAE) / Naive_MAE) × 100\n",
    "    2. MSE improvement %: ((Naive_MSE - Model_MSE) / Naive_MSE) × 100  \n",
    "    3. Prediction Score: 1 − L(model)/L(naive) where L is MSE loss\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get Naive baseline values\n",
    "    naive_row = results_df[results_df['model'] == 'Naive']\n",
    "    if len(naive_row) == 0:\n",
    "        print(f\"Warning: No Naive model found in {dataset_name}\")\n",
    "        return None\n",
    "        \n",
    "    naive_mae = naive_row['mae'].iloc[0]\n",
    "    naive_mse = naive_row['mse'].iloc[0]\n",
    "    \n",
    "    # Calculate metrics for all models\n",
    "    metrics_df = results_df.copy()\n",
    "    \n",
    "    # 1. MAE improvement percentage\n",
    "    metrics_df['mae_improvement_pct'] = ((naive_mae - metrics_df['mae']) / naive_mae) * 100\n",
    "    \n",
    "    # 2. MSE improvement percentage  \n",
    "    metrics_df['mse_improvement_pct'] = ((naive_mse - metrics_df['mse']) / naive_mse) * 100\n",
    "    \n",
    "    # 3. Prediction Score: 1 - L(model)/L(naive)\n",
    "    metrics_df['prediction_score'] = 1 - (metrics_df['mse'] / naive_mse)\n",
    "    \n",
    "    # Add dataset identifier\n",
    "    metrics_df['dataset'] = dataset_name\n",
    "    \n",
    "    return metrics_df\n",
    "\n",
    "print(\"Calculating performance metrics for all datasets...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calculate metrics for SHORT forecasting (Context=16, Prediction=8)\n",
    "short_metrics_70 = calculate_performance_metrics(resultsShort70, \"Short_70N\")\n",
    "short_metrics_700 = calculate_performance_metrics(resultsShort700, \"Short_700N\") \n",
    "short_metrics_7000 = calculate_performance_metrics(resultsShort7000, \"Short_7000N\")\n",
    "\n",
    "# Calculate metrics for LONG forecasting (Context=48, Prediction=16)\n",
    "long_metrics_70 = calculate_performance_metrics(resultsLong70, \"Long_70N\")\n",
    "long_metrics_700 = calculate_performance_metrics(resultsLong700, \"Long_700N\")\n",
    "long_metrics_7000 = calculate_performance_metrics(resultsLong7000, \"Long_7000N\")\n",
    "\n",
    "print(\"Metrics calculated successfully!\")\n",
    "print(\"\\nFormulas used:\")\n",
    "print(\"1. MAE Improvement %: ((Naive_MAE - Model_MAE) / Naive_MAE) × 100\")\n",
    "print(\"2. MSE Improvement %: ((Naive_MSE - Model_MSE) / Naive_MSE) × 100\") \n",
    "print(\"3. Prediction Score: 1 − L(model)/L(naive) where L = MSE loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cellnrnny",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERFORMANCE METRICS SUMMARY\n",
      "================================================================================\n",
      "Context=16, Prediction=8 (SHORT) vs Context=48, Prediction=16 (LONG)\n",
      "All metrics calculated relative to Naive baseline within each dataset\n",
      "================================================================================\n",
      "\n",
      "SHORT Forecasting Performance (Context=16, Prediction=8)\n",
      "----------------------------------------------------------------------\n",
      "      Model     Dataset  MAE Improv %  MSE Improv %  Pred Score\n",
      "       Mean   Short_70N        -24.72        -56.85     -0.5685\n",
      "    TSMixer   Short_70N          8.82          4.22      0.0422\n",
      "       POCO   Short_70N         13.37          6.84      0.0684\n",
      "     Linear   Short_70N          4.23         -5.76     -0.0576\n",
      "    DLinear   Short_70N          9.24          3.16      0.0316\n",
      "   Informer   Short_70N        -13.78        -54.56     -0.5456\n",
      "Transformer   Short_70N         -4.42        -21.72     -0.2172\n",
      "       Mean  Short_700N         10.41         12.56      0.1256\n",
      "    TSMixer  Short_700N         18.79         28.38      0.2838\n",
      "       POCO  Short_700N         21.74         32.09      0.3209\n",
      "     Linear  Short_700N         15.00         23.17      0.2317\n",
      "    DLinear  Short_700N         17.32         26.81      0.2681\n",
      "   Informer  Short_700N          9.83          5.42      0.0542\n",
      "Transformer  Short_700N         10.55          7.78      0.0778\n",
      "       Mean Short_7000N         20.82         34.82      0.3482\n",
      "    TSMixer Short_7000N         23.27         38.92      0.3892\n",
      "       POCO Short_7000N         25.65         41.86      0.4186\n",
      "     Linear Short_7000N         20.66         35.32      0.3532\n",
      "    DLinear Short_7000N         22.60         38.28      0.3828\n",
      "   Informer Short_7000N         22.08         34.40      0.3440\n",
      "Transformer Short_7000N         22.25         34.96      0.3496\n",
      "\n",
      "LONG Forecasting Performance (Context=48, Prediction=16)\n",
      "----------------------------------------------------------------------\n",
      "      Model    Dataset  MAE Improv %  MSE Improv %  Pred Score\n",
      "       Mean   Long_70N        -25.33        -41.41     -0.4141\n",
      "    TSMixer   Long_70N         11.09          9.58      0.0958\n",
      "       POCO   Long_70N         14.91         11.35      0.1135\n",
      "     Linear   Long_70N          9.05          6.35      0.0635\n",
      "    DLinear   Long_70N         11.95         10.73      0.1073\n",
      "   Informer   Long_70N        -12.47        -31.94     -0.3194\n",
      "Transformer   Long_70N         -3.03        -10.88     -0.1088\n",
      "       Mean  Long_700N         11.53         17.51      0.1751\n",
      "    TSMixer  Long_700N         21.22         31.68      0.3168\n",
      "       POCO  Long_700N         23.64         32.75      0.3275\n",
      "     Linear  Long_700N         18.66         29.98      0.2998\n",
      "    DLinear  Long_700N         19.30         30.85      0.3085\n",
      "   Informer  Long_700N         14.07         16.51      0.1651\n",
      "Transformer  Long_700N         14.80         16.66      0.1666\n",
      "       Mean Long_7000N         22.66         38.41      0.3841\n",
      "    TSMixer Long_7000N         25.72         42.48      0.4248\n",
      "       POCO Long_7000N         26.60         43.13      0.4313\n",
      "     Linear Long_7000N         24.19         40.81      0.4081\n",
      "    DLinear Long_7000N         24.47         41.20      0.4120\n",
      "   Informer Long_7000N         24.11         38.34      0.3834\n",
      "Transformer Long_7000N         24.32         38.83      0.3883\n",
      "\n",
      "\n",
      "NAIVE BASELINES (Reference Values)\n",
      "--------------------------------------------------\n",
      "    Dataset  Naive MAE  Naive MSE\n",
      "  Short_70N     0.0654     0.0136\n",
      " Short_700N     0.1112     0.0265\n",
      "Short_7000N     0.1517     0.0417\n",
      "   Long_70N     0.0827     0.0222\n",
      "  Long_700N     0.1205     0.0321\n",
      " Long_7000N     0.1564     0.0448\n"
     ]
    }
   ],
   "source": [
    "# Combine all metrics into comprehensive tables\n",
    "import pandas as pd\n",
    "\n",
    "# Combine all datasets\n",
    "all_metrics = pd.concat([\n",
    "    short_metrics_70, short_metrics_700, short_metrics_7000,\n",
    "    long_metrics_70, long_metrics_700, long_metrics_7000\n",
    "], ignore_index=True)\n",
    "\n",
    "# Separate short and long for better organization\n",
    "short_metrics = pd.concat([short_metrics_70, short_metrics_700, short_metrics_7000], ignore_index=True)\n",
    "long_metrics = pd.concat([long_metrics_70, long_metrics_700, long_metrics_7000], ignore_index=True)\n",
    "\n",
    "print(\"PERFORMANCE METRICS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Context=16, Prediction=8 (SHORT) vs Context=48, Prediction=16 (LONG)\")\n",
    "print(\"All metrics calculated relative to Naive baseline within each dataset\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Function to create formatted table\n",
    "def create_performance_table(metrics_df, title):\n",
    "    # Exclude Naive from display (it's always 0% improvement, score=0)\n",
    "    display_df = metrics_df[metrics_df['model'] != 'Naive'].copy()\n",
    "    \n",
    "    # Select and rename columns for display\n",
    "    table_df = display_df[['model', 'dataset', 'mae_improvement_pct', 'mse_improvement_pct', 'prediction_score']].copy()\n",
    "    table_df.columns = ['Model', 'Dataset', 'MAE Improv %', 'MSE Improv %', 'Pred Score']\n",
    "    \n",
    "    # Round values for readability\n",
    "    table_df['MAE Improv %'] = table_df['MAE Improv %'].round(2)\n",
    "    table_df['MSE Improv %'] = table_df['MSE Improv %'].round(2)\n",
    "    table_df['Pred Score'] = table_df['Pred Score'].round(4)\n",
    "    \n",
    "    print(f\"\\n{title}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(table_df.to_string(index=False))\n",
    "    \n",
    "    return table_df\n",
    "\n",
    "# Create tables for SHORT and LONG separately\n",
    "short_table = create_performance_table(short_metrics, \"SHORT Forecasting Performance (Context=16, Prediction=8)\")\n",
    "long_table = create_performance_table(long_metrics, \"LONG Forecasting Performance (Context=48, Prediction=16)\")\n",
    "\n",
    "print(f\"\\n\\nNAIVE BASELINES (Reference Values)\")\n",
    "print(\"-\" * 50)\n",
    "naive_baselines = all_metrics[all_metrics['model'] == 'Naive'][['dataset', 'mae', 'mse']].copy()\n",
    "naive_baselines.columns = ['Dataset', 'Naive MAE', 'Naive MSE']\n",
    "naive_baselines['Naive MAE'] = naive_baselines['Naive MAE'].round(4)\n",
    "naive_baselines['Naive MSE'] = naive_baselines['Naive MSE'].round(4)\n",
    "print(naive_baselines.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sndptq9s7tp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ADVANCED ANALYSIS\n",
      "============================================================\n",
      "\n",
      "SHORT Forecasting - Best Performers:\n",
      "---------------------------------------------\n",
      "\n",
      "Short_70N:\n",
      "  Best MAE Improv: POCO         (+13.37%)\n",
      "  Best MSE Improv: POCO         ( +6.84%)\n",
      "  Best Pred Score: POCO         ( 0.0684)\n",
      "\n",
      "Short_700N:\n",
      "  Best MAE Improv: POCO         (+21.74%)\n",
      "  Best MSE Improv: POCO         (+32.09%)\n",
      "  Best Pred Score: POCO         ( 0.3209)\n",
      "\n",
      "Short_7000N:\n",
      "  Best MAE Improv: POCO         (+25.65%)\n",
      "  Best MSE Improv: POCO         (+41.86%)\n",
      "  Best Pred Score: POCO         ( 0.4186)\n",
      "\n",
      "LONG Forecasting - Best Performers:\n",
      "---------------------------------------------\n",
      "\n",
      "Long_70N:\n",
      "  Best MAE Improv: POCO         (+14.91%)\n",
      "  Best MSE Improv: POCO         (+11.35%)\n",
      "  Best Pred Score: POCO         ( 0.1135)\n",
      "\n",
      "Long_700N:\n",
      "  Best MAE Improv: POCO         (+23.64%)\n",
      "  Best MSE Improv: POCO         (+32.75%)\n",
      "  Best Pred Score: POCO         ( 0.3275)\n",
      "\n",
      "Long_7000N:\n",
      "  Best MAE Improv: POCO         (+26.60%)\n",
      "  Best MSE Improv: POCO         (+43.13%)\n",
      "  Best Pred Score: POCO         ( 0.4313)\n",
      "\n",
      "\n",
      "OVERALL PERFORMANCE STATISTICS\n",
      "==================================================\n",
      "\n",
      "SHORT Forecasting Summary:\n",
      "MAE Improvement %  - Mean:  12.08, Std:  12.91, Max:  25.65\n",
      "MSE Improvement %  - Mean:  12.86, Std:  28.51, Max:  41.86\n",
      "Prediction Score   - Mean: 0.1286, Std: 0.2851, Max: 0.4186\n",
      "\n",
      "LONG Forecasting Summary:\n",
      "MAE Improvement %  - Mean:  14.36, Std:  13.36, Max:  26.60\n",
      "MSE Improvement %  - Mean:  19.66, Std:  23.85, Max:  43.13\n",
      "Prediction Score   - Mean: 0.1966, Std: 0.2385, Max: 0.4313\n",
      "\n",
      "\n",
      "MODEL CONSISTENCY ACROSS DATASETS\n",
      "==================================================\n",
      "\n",
      "SHORT - Model Consistency (Mean ± Std across datasets):\n",
      "Model        MAE Improv %     MSE Improv %     Pred Score\n",
      "------------------------------------------------------------\n",
      "DLinear       16.39± 6.73     22.75±17.91    0.2270±0.1790\n",
      "Informer       6.04±18.23     -4.91±45.37    -0.0490±0.4540\n",
      "Linear        13.30± 8.35     17.58±21.11    0.1760±0.2110\n",
      "Mean           2.17±23.86     -3.16±47.81    -0.0320±0.4780\n",
      "POCO          20.25± 6.28     26.93±18.07    0.2690±0.1810\n",
      "TSMixer       16.96± 7.39     23.84±17.79    0.2380±0.1780\n",
      "Transformer    9.46±13.37      7.01±28.35    0.0700±0.2830\n",
      "\n",
      "LONG - Model Consistency (Mean ± Std across datasets):\n",
      "Model        MAE Improv %     MSE Improv %     Pred Score\n",
      "------------------------------------------------------------\n",
      "DLinear       18.57± 6.29     27.59±15.50    0.2760±0.1550\n",
      "Informer       8.57±18.90      7.64±35.97    0.0760±0.3600\n",
      "Linear        17.30± 7.66     25.71±17.62    0.2570±0.1760\n",
      "Mean           2.95±25.12      4.84±41.40    0.0480±0.4140\n",
      "POCO          21.72± 6.08     29.08±16.20    0.2910±0.1620\n",
      "TSMixer       19.34± 7.49     27.91±16.77    0.2790±0.1680\n",
      "Transformer   12.03±13.89     14.87±24.91    0.1490±0.2490\n"
     ]
    }
   ],
   "source": [
    "# Advanced Analysis: Best performing models and patterns\n",
    "print(\"\\nADVANCED ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def analyze_best_performers(metrics_df, forecasting_type):\n",
    "    \"\"\"Analyze best performing models across different metrics\"\"\"\n",
    "    non_naive = metrics_df[metrics_df['model'] != 'Naive'].copy()\n",
    "    \n",
    "    print(f\"\\n{forecasting_type} Forecasting - Best Performers:\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    for dataset in non_naive['dataset'].unique():\n",
    "        data_subset = non_naive[non_naive['dataset'] == dataset]\n",
    "        \n",
    "        # Best by each metric\n",
    "        best_mae = data_subset.loc[data_subset['mae_improvement_pct'].idxmax()]\n",
    "        best_mse = data_subset.loc[data_subset['mse_improvement_pct'].idxmax()] \n",
    "        best_pred = data_subset.loc[data_subset['prediction_score'].idxmax()]\n",
    "        \n",
    "        print(f\"\\n{dataset}:\")\n",
    "        print(f\"  Best MAE Improv: {best_mae['model']:12} ({best_mae['mae_improvement_pct']:+6.2f}%)\")\n",
    "        print(f\"  Best MSE Improv: {best_mse['model']:12} ({best_mse['mse_improvement_pct']:+6.2f}%)\")  \n",
    "        print(f\"  Best Pred Score: {best_pred['model']:12} ({best_pred['prediction_score']:7.4f})\")\n",
    "\n",
    "# Analyze both forecasting types\n",
    "analyze_best_performers(short_metrics, \"SHORT\")\n",
    "analyze_best_performers(long_metrics, \"LONG\")\n",
    "\n",
    "# Overall summary statistics\n",
    "print(f\"\\n\\nOVERALL PERFORMANCE STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def print_summary_stats(metrics_df, name):\n",
    "    non_naive = metrics_df[metrics_df['model'] != 'Naive']\n",
    "    \n",
    "    mae_stats = non_naive['mae_improvement_pct'].describe()\n",
    "    mse_stats = non_naive['mse_improvement_pct'].describe()  \n",
    "    pred_stats = non_naive['prediction_score'].describe()\n",
    "    \n",
    "    print(f\"\\n{name} Forecasting Summary:\")\n",
    "    print(f\"MAE Improvement %  - Mean: {mae_stats['mean']:6.2f}, Std: {mae_stats['std']:6.2f}, Max: {mae_stats['max']:6.2f}\")\n",
    "    print(f\"MSE Improvement %  - Mean: {mse_stats['mean']:6.2f}, Std: {mse_stats['std']:6.2f}, Max: {mse_stats['max']:6.2f}\")\n",
    "    print(f\"Prediction Score   - Mean: {pred_stats['mean']:6.4f}, Std: {pred_stats['std']:6.4f}, Max: {pred_stats['max']:6.4f}\")\n",
    "\n",
    "print_summary_stats(short_metrics, \"SHORT\")\n",
    "print_summary_stats(long_metrics, \"LONG\")\n",
    "\n",
    "# Model consistency analysis\n",
    "print(f\"\\n\\nMODEL CONSISTENCY ACROSS DATASETS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def analyze_consistency(metrics_df, name):\n",
    "    non_naive = metrics_df[metrics_df['model'] != 'Naive']\n",
    "    \n",
    "    # Calculate coefficient of variation for each model across datasets\n",
    "    model_consistency = non_naive.groupby('model').agg({\n",
    "        'mae_improvement_pct': ['mean', 'std'],\n",
    "        'mse_improvement_pct': ['mean', 'std'],\n",
    "        'prediction_score': ['mean', 'std']\n",
    "    }).round(3)\n",
    "    \n",
    "    print(f\"\\n{name} - Model Consistency (Mean ± Std across datasets):\")\n",
    "    print(\"Model        MAE Improv %     MSE Improv %     Pred Score\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for model in model_consistency.index:\n",
    "        mae_mean = model_consistency.loc[model, ('mae_improvement_pct', 'mean')]\n",
    "        mae_std = model_consistency.loc[model, ('mae_improvement_pct', 'std')]\n",
    "        mse_mean = model_consistency.loc[model, ('mse_improvement_pct', 'mean')]\n",
    "        mse_std = model_consistency.loc[model, ('mse_improvement_pct', 'std')]\n",
    "        pred_mean = model_consistency.loc[model, ('prediction_score', 'mean')]\n",
    "        pred_std = model_consistency.loc[model, ('prediction_score', 'std')]\n",
    "        \n",
    "        print(f\"{model:12} {mae_mean:6.2f}±{mae_std:5.2f}    {mse_mean:6.2f}±{mse_std:5.2f}    {pred_mean:6.4f}±{pred_std:6.4f}\")\n",
    "\n",
    "analyze_consistency(short_metrics, \"SHORT\")\n",
    "analyze_consistency(long_metrics, \"LONG\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
